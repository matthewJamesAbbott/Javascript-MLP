<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Multi-Layer Perceptron Trainer with Evaluation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 2rem;
        }

        input,
        select,
        textarea,
        button {
            margin: 0.5rem 0;
        }

        .section {
            margin-bottom: 2rem;
            border-bottom: 1px solid #ccc;
            padding-bottom: 1rem;
        }

        label {
            display: block;
            margin-top: 0.5rem;
        }

        .results {
            background-color: #f5f5f5;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 5px;
        }

        .metric {
            margin: 0.5rem 0;
            font-weight: bold;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1rem 0;
        }

        th,
        td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }

        .epoch-controls {
            display: flex;
            gap: 1rem;
            align-items: center;
            flex-wrap: wrap;
        }

        .epoch-info {
            font-style: italic;
            color: #666;
            margin-top: 0.5rem;
        }
    </style>
</head>

<body>
    <h1>Multi-Layer Perceptron Trainer with Evaluation</h1>

    <div class="section">
        <h2>Network Configuration</h2>
        <label>Input Size: <input type="number" id="inputSize" value="10" min="1"></label>
        <label>Hidden Layer Sizes (comma separated): <input type="text" id="hiddenSizes" value="8,8,8"></label>
        <label>Output Size: <input type="number" id="outputSize" value="3" min="1"></label>
        <label>Hidden Layer Activation:
            <select id="hiddenActivation">
                <option value="sigmoid" selected>Sigmoid</option>
                <option value="tanh">Tanh</option>
                <option value="relu">ReLU</option>
            </select>
        </label>
        <label>Output Layer Activation:
            <select id="outputActivation">
                <option value="softmax" selected>Softmax</option>
                <option value="sigmoid">Sigmoid</option>
                <option value="tanh">Tanh</option>
                <option value="relu">ReLU</option>
            </select>
        </label>
        <label>Learning Rate: <input type="number" id="learningRate" value="0.1" step="0.01" min="0.001"
                max="1"></label>
        <button onclick="createNetwork()">Create Network</button>
        <span id="networkStatus" style="margin-left:1rem;"></span>
    </div>

    <div class="section">
        <h2>Training Configuration</h2>
        <div class="epoch-controls">
            <label>Training Epochs:
                <input type="number" id="trainingEpochs" value="50" min="1" max="10000" step="1">
            </label>
            <label>Validation Epochs:
                <input type="number" id="validationEpochs" value="30" min="1" max="1000" step="1">
            </label>
            <label>Batch Size:
                <select id="batchSize">
                    <option value="1">1 (Online Learning)</option>
                    <option value="32" selected>32</option>
                    <option value="64">64</option>
                    <option value="128">128</option>
                    <option value="all">Full Batch</option>
                </select>
            </label>
        </div>
        <div class="epoch-info">
            ℹ️ Training Epochs: Full passes through training data for regular training<br>
            ℹ️ Validation Epochs: Epochs used for each fold in K-Fold cross-validation<br>
            ℹ️ Batch Size: Number of samples processed before updating weights
        </div>
    </div>

    <div class="section">
        <h2>Load Training Data</h2>
        <input type="file" id="csvFile" accept=".csv">
        <button onclick="loadCSV()">Load CSV</button>
        <span id="dataStatus"></span>
        <br>
        <label>Or paste CSV data:</label>
        <textarea id="csvPaste" rows="5" cols="60"></textarea>
        <button onclick="loadCSVText()">Load Pasted CSV</button>
    </div>

    <div class="section">
        <h2>Generate Test Data</h2>
        <label>Number of Samples per Class: <input type="number" id="samplesPerClass" value="2500" min="100"></label>
        <button onclick="generateTestData()">Generate Test Data</button>
        <span id="generateStatus"></span>
    </div>

    <div class="section">
        <h2>Model Evaluation</h2>
        <label>Number of Folds: <input type="number" id="numFolds" value="10" min="2" max="20"></label>
        <button onclick="runKFoldValidation()">Run K-Fold Cross Validation</button>
        <button onclick="runAllMetrics()">Run All Evaluation Metrics</button>
        <div id="evaluationResults"></div>
    </div>

    <div class="section">
        <h2>Save/Load Model</h2>
        <button onclick="saveModel()">Save Model</button>
        <button onclick="loadModel()">Load Model</button>
        <input type="file" id="modelFile" accept=".json">
        <span id="modelStatus"></span>
    </div>

    <div class="section">
        <h2>Train Network</h2>
        <button onclick="trainNetwork()">Train Network</button>
        <button onclick="trainNetworkWithProgress()">Train with Progress</button>
        <div id="trainStatus"></div>
        <div id="trainingProgress"></div>
    </div>

    <div class="section">
        <h2>Predict</h2>
        <label>Input (comma separated): <input type="text" id="predictInput"
                value="0.19,0.21,0.32,0.43,0.54,0.65,0.77,0.88,0.99,0.01"></label>
        <button onclick="predictInput()">Predict</button>
        <div id="predictOutput"></div>
    </div>

    <script>
        // Activation Functions
        class ActivationFunctions {
            /**
             * The softmax function is used for output layer activation.
             * @param {Array} x - The input array to the softmax function.
             * @returns {Array} The output of the softmax function.
             */
            static softmax(x) {
                if (!(x instanceof Array)) return null;

                let sum = 0;
                for (let i = 0; i < x.length; i++) {
                    sum += Math.exp(x[i]);
                }
                let result = [];
                for (let i = 0; i < x.length; i++) {
                    result[i] = Math.exp(x[i]) / sum;
                }
                return result;
            }

            /**
             * The derivative of the softmax function.
             * @param {Array} labels - The output of the softmax function.
             * @param {Array} x - The input array to the derivative of the softmax function.
             * @returns {Array} The derivative of the softmax function.
             */
            static dSoftmax(labels, x) {
                if (!(x instanceof Array)) return null;

                let result = [];
                for (let i = 0; i < x.length; i++) {
                    result[i] = labels[i] - x[i];
                }
                return result;
            }

            /**
             * The sigmoid function is used for hidden layer activation.
             * @param {Number} x - The input to the sigmoid function.
             * @returns {Number} The output of the sigmoid function.
             */
            static sigmoid(x) {
                return 1 / (1 + Math.exp(-x));
            }

            /**
             * The derivative of the sigmoid function.
             * @param {Number} x - The input to the derivative of the sigmoid function.
             * @returns {Number} The derivative of the sigmoid function.
             */
            static dSigmoid(x) {
                return x * (1 - x);
            }

            /**
             * The tanh function is used for hidden layer activation.
             * @param {Number} x - The input to the tanh function.
             * @returns {Number} The output of the tanh function.
             */
            static tanh(x) {
                return Math.tanh(x);
            }

            /**
             * The derivative of the tanh function.
             * @param {Number} x - The input to the derivative of the tanh function.
             * @returns {Number} The derivative of the tanh function.
             */
            static dTanh(x) {
                return 1 - (x * x);
            }

            /**
             * The ReLU function is used for hidden layer activation.
             * @param {Number} x - The input to the ReLU function.
             * @returns {Number} The output of the ReLU function.
             */
            static relu(x) {
                return Math.max(0, x);
            }

            /**
             * The derivative of the ReLU function.
             * @param {Number} x - The input to the derivative of the ReLU function.
             * @returns {Number} The derivative of the ReLU function.
             */
            static dRelu(x) {
                return x > 0 ? 1 : 0;
            }

            /**
             * Applies the specified activation function to the input.
             * @param {String} activationType - The type of activation function to apply.
             * @param {Array} x - The input to the activation function.
             * @returns {Array} The output of the activation function.
             */
            static apply(activationType, x) {
                switch (activationType) {
                    case 'sigmoid': return this.sigmoid(x);
                    case 'tanh': return this.tanh(x);
                    case 'relu': return this.relu(x);
                    case 'softmax': return this.softmax(x);
                    default: return this.sigmoid(x);
                }
            }

            /**
             * Applies the derivative of the specified activation function to the input.
             * @param {String} activationType - The type of activation function to apply.
             * @param {Number} x - The input to the derivative of the activation function.
             * @param {Array} target - The target output of the derivative of the activation function.
             * @returns {Number} The derivative of the activation function.
             */
            static applyDerivative(activationType, x, target = null) {
                switch (activationType) {
                    case 'sigmoid': return this.dSigmoid(x);
                    case 'tanh': return this.dTanh(x);
                    case 'relu': return this.dRelu(x);
                    case 'softmax': return target ? this.dSoftmax(target, x) : x;
                    default: return this.dSigmoid(x);
                }
            }
        }

        // Evaluation Functions with Epoch Support
        class MLEvaluation {
            /**
             * Performs K-Fold Cross Validation on the MLP.
             * @param {Array} data - The training data.
             * @param {Number} numFolds - The number of folds to perform.
             * @param {TMultiLayerPerceptron} mlp - The MLP to evaluate.
             * @param {Number} epochs - The number of epochs to train for each fold.
             * @returns {Number} The average accuracy of the MLP on the K-Fold Cross Validation.
             */
            static kFoldCrossValidation(data, numFolds, mlp, epochs = null) {
                const numSamples = data.length;
                const foldSize = Math.floor(numSamples / numFolds);
                let sumAccuracy = 0;

                // Use validation epochs if provided, otherwise use training epochs
                const validationEpochs = epochs || parseInt(document.getElementById("validationEpochs").value);

                for (let i = 0; i < numFolds; i++) {
                    // Create a fresh copy of the MLP for each fold
                    const foldMLP = this.cloneMLP(mlp);

                    // Create test set for fold i
                    const testSet = [];
                    for (let j = 0; j < foldSize; j++) {
                        testSet.push(data[i * foldSize + j]);
                    }

                    // Create training set for fold i (all data except test set)
                    const trainSet = [];
                    for (let j = 0; j < numSamples; j++) {
                        if (j >= i * foldSize && j < (i + 1) * foldSize) {
                            continue; // Skip test set data
                        }
                        trainSet.push(data[j]);
                    }

                    // Train MLP on training set for specified epochs
                    for (let epoch = 0; epoch < validationEpochs; epoch++) {
                        for (let k = 0; k < trainSet.length; k++) {
                            foldMLP.train(trainSet[k].input, trainSet[k].target);
                        }
                    }

                    // Evaluate accuracy on test set
                    let correctPredictions = 0;
                    for (let j = 0; j < testSet.length; j++) {
                        const prediction = foldMLP.predict(testSet[j].input);
                        const predictedClass = this.getMaxIndex(prediction);
                        const actualClass = this.getMaxIndex(testSet[j].target);

                        if (predictedClass === actualClass) {
                            correctPredictions++;
                        }
                    }

                    sumAccuracy += correctPredictions;
                }

                return sumAccuracy / numSamples;
            }

            /**
             * Clones the MLP for independent training.
             * @param {TMultiLayerPerceptron} mlp - The MLP to clone.
             * @returns {TMultiLayerPerceptron} The cloned MLP.
             */
            static cloneMLP(mlp) {
                // Create a deep copy of the MLP for independent training
                const jsonStr = mlp.toJSON();
                return TMultiLayerPerceptron.fromJSON(jsonStr);
            }

            /**
             * Trains the MLP with batches.
             * @param {TMultiLayerPerceptron} mlp - The MLP to train.
             * @param {Array} data - The training data.
             * @param {Number} epochs - The number of epochs to train for.
             * @param {String} batchSize - The batch size to use.
             * @param {Function} progressCallback - The callback function to call after each epoch.
             */
            static trainWithBatches(mlp, data, epochs, batchSize, progressCallback = null) {
                const totalSamples = data.length;
                let actualBatchSize = batchSize === 'all' ? totalSamples : parseInt(batchSize);

                for (let epoch = 0; epoch < epochs; epoch++) {
                    // Shuffle data for each epoch
                    const shuffledData = [...data].sort(() => Math.random() - 0.5);

                    for (let i = 0; i < totalSamples; i += actualBatchSize) {
                        const batch = shuffledData.slice(i, Math.min(i + actualBatchSize, totalSamples));

                        // Train on batch
                        for (let j = 0; j < batch.length; j++) {
                            mlp.train(batch[j].input, batch[j].target);
                        }
                    }

                    // Call progress callback if provided
                    if (progressCallback && epoch % 10 === 0) {
                        progressCallback(epoch + 1, epochs);
                    }
                }

                if (progressCallback) {
                    progressCallback(epochs, epochs); // Final update
                }
            }

            /**
             * Calculates the precision score of the MLP.
             * @param {Array} data - The data to evaluate.
             * @param {TMultiLayerPerceptron} mlp - The MLP to evaluate.
             * @param {Number} classIndex - The class index to evaluate.
             * @returns {Number} The precision score of the MLP.
             */
            static precisionScore(data, mlp, classIndex = 0) {
                let truePositives = 0;
                let falsePositives = 0;

                for (let i = 0; i < data.length; i++) {
                    const prediction = mlp.predict(data[i].input);
                    const predictedClass = this.getMaxIndex(prediction);
                    const actualClass = this.getMaxIndex(data[i].target);

                    if (predictedClass === classIndex) {
                        if (actualClass === classIndex) {
                            truePositives++;
                        } else {
                            falsePositives++;
                        }
                    }
                }

                if (truePositives + falsePositives === 0) {
                    return 0;
                }
                return truePositives / (truePositives + falsePositives);
            }

            /**
             * Calculates the recall score of the MLP.
             * @param {Array} data - The data to evaluate.
             * @param {TMultiLayerPerceptron} mlp - The MLP to evaluate.
             * @param {Number} classIndex - The class index to evaluate.
             * @returns {Number} The recall score of the MLP.
             */
            static recallScore(data, mlp, classIndex = 0) {
                let truePositives = 0;
                let falseNegatives = 0;

                for (let i = 0; i < data.length; i++) {
                    const prediction = mlp.predict(data[i].input);
                    const predictedClass = this.getMaxIndex(prediction);
                    const actualClass = this.getMaxIndex(data[i].target);

                    if (actualClass === classIndex) {
                        if (predictedClass === classIndex) {
                            truePositives++;
                        } else {
                            falseNegatives++;
                        }
                    }
                }

                if (truePositives + falseNegatives === 0) {
                    return 0;
                }
                return truePositives / (truePositives + falseNegatives);
            }

            /**
             * Calculates the F1 score of the MLP.
             * @param {Number} precision - The precision score of the MLP.
             * @param {Number} recall - The recall score of the MLP.
             * @returns {Number} The F1 score of the MLP.
             */
            static f1Score(precision, recall) {
                if (precision + recall === 0) {
                    return 0;
                }
                return 2 * (precision * recall) / (precision + recall);
            }

            /**
             * Gets the index of the maximum value in the array.
             * @param {Array} array - The array to find the maximum value in.
             * @returns {Number} The index of the maximum value in the array.
             */
            static getMaxIndex(array) {
                let maxIndex = 0;
                for (let i = 1; i < array.length; i++) {
                    if (array[i] > array[maxIndex]) {
                        maxIndex = i;
                    }
                }
                return maxIndex;
            }

            /**
             * Generates test data for the MLP.
             * @param {Number} samplesPerClass - The number of samples per class.
             * @param {Number} inputSize - The size of the input.
             * @param {Number} numClasses - The number of classes.
             * @returns {Array} The generated test data.
             */
            static generateTestData(samplesPerClass, inputSize, numClasses) {
                const data = [];

                for (let classIndex = 0; classIndex < numClasses; classIndex++) {
                    for (let i = 0; i < samplesPerClass; i++) {
                        const input = [];
                        const target = new Array(numClasses).fill(0);
                        target[classIndex] = 1;

                        // Generate different patterns for each class
                        for (let j = 0; j < inputSize; j++) {
                            switch (classIndex) {
                                case 0: // Class 0: low values (0-0.5)
                                    input.push(Math.random() * 0.5);
                                    break;
                                case 1: // Class 1: high values (0.5-1.0)
                                    input.push(0.5 + Math.random() * 0.5);
                                    break;
                                case 2: // Class 2: mixed pattern
                                    input.push(j % 2 === 0 ? Math.random() * 0.5 : 0.5 + Math.random() * 0.5);
                                    break;
                                default: // Additional classes: random patterns
                                    input.push(Math.random());
                                    break;
                            }
                        }

                        data.push(new TDataPoint(input, target));
                    }
                }

                // Shuffle the data
                for (let i = data.length - 1; i > 0; i--) {
                    const j = Math.floor(Math.random() * (i + 1));
                    [data[i], data[j]] = [data[j], data[i]];
                }

                return data;
            }
        }

        class TDataPoint {
            /**
             * Creates a new TDataPoint object.
             * @param {Array} input - The input of the data point.
             * @param {Array} target - The target of the data point.
             */
            constructor(input, target) {
                this.input = input;
                this.target = target;
            }
        }

        class TNeuron {
            /**
             * Creates a new TNeuron object.
             */
            constructor() {
                this.weights = [];
                this.bias = 0;
                this.output = 0;
                this.error = 0;
            }
        }

        class TLayer {
            /**
             * Creates a new TLayer object.
             */
            constructor() {
                this.neurons = [];
                this.activationType = 'sigmoid';
            }
        }

        class TMultiLayerPerceptron {
            /**
             * Creates a new TMultiLayerPerceptron object.
             * @param {Number} inputSize - The size of the input.
             * @param {Array} hiddenSizes - The sizes of the hidden layers.
             * @param {Number} outputSize - The size of the output.
             * @param {String} hiddenActivation - The activation function of the hidden layers.
             * @param {String} outputActivation - The activation function of the output layer.
             */
            constructor(inputSize, hiddenSizes, outputSize, hiddenActivation = 'sigmoid', outputActivation = 'sigmoid') {
                this.learningRate = 0.1;
                this.inputLayer = new TLayer();
                this.hiddenLayers = [];
                this.outputLayer = new TLayer();
                this.hiddenActivation = hiddenActivation;
                this.outputActivation = outputActivation;
                this.initialize(inputSize, hiddenSizes, outputSize);
            }

            /**
             * Initializes the weights of the neurons in the input layer.
             * @param {Number} inputSize - The size of the input.
             * @returns {Array} The weights of the neurons in the input layer.
             */
            initializeWeights(inputSize) {
                const limit = Math.sqrt(6 / inputSize);
                return new Array(inputSize).fill(0).map(() =>
                    (Math.random() * 2 - 1) * limit
                );
            }

            /**
             * Initializes the MLP.
             * @param {Number} inputSize - The size of the input.
             * @param {Array} hiddenSizes - The sizes of the hidden layers.
             * @param {Number} outputSize - The size of the output.
             */
            initialize(inputSize, hiddenSizes, outputSize) {
                // Input layer
                this.inputLayer.neurons = [];
                for (let i = 0; i < inputSize; i++) {
                    this.inputLayer.neurons.push(new TNeuron());
                    this.inputLayer.neurons[i].output = 0;
                }

                // Hidden layers
                this.hiddenLayers = [];
                let prevSize = inputSize;
                hiddenSizes.forEach((size) => {
                    let layer = new TLayer();
                    layer.activationType = this.hiddenActivation;
                    for (let i = 0; i < size; i++) {
                        let neuron = new TNeuron();
                        neuron.weights = this.initializeWeights(prevSize);
                        neuron.bias = (Math.random() * 2 - 1) * 0.1;
                        layer.neurons.push(neuron);
                    }
                    this.hiddenLayers.push(layer);
                    prevSize = size;
                });

                // Output layer
                this.outputLayer = new TLayer();
                this.outputLayer.activationType = this.outputActivation;
                for (let i = 0; i < outputSize; i++) {
                    let neuron = new TNeuron();
                    neuron.weights = this.initializeWeights(prevSize);
                    neuron.bias = (Math.random() * 2 - 1) * 0.1;
                    this.outputLayer.neurons.push(neuron);
                }
            }

            /**
             * Feeds the input forward through the MLP.
             * @param {Array} input - The input to the MLP.
             * @returns {Array} The output of the MLP.
             */
            feedForward(input) {
                for (let i = 0; i < this.inputLayer.neurons.length; i++) {
                    this.inputLayer.neurons[i].output = input[i];
                }

                /**
                * Initialize the previous outputs with the input.
                */
                let prevOutputs = input;

                /**
                 * Propagate the output through the hidden layers.
                 */
                for (let k = 0; k < this.hiddenLayers.length; k++) {
                    /**
                     * Initialize the current outputs array.
                     */
                    let curOutputs = [];

                    /**
                     * Calculate the output of each neuron in the current layer.
                     */
                    for (let i = 0; i < this.hiddenLayers[k].neurons.length; i++) {
                        /**
                         * Initialize the sum of the weighted inputs to the neuron.
                         */
                        let sum = this.hiddenLayers[k].neurons[i].bias;

                        /**
                         * Calculate the weighted sum of the inputs to the neuron.
                         */
                        for (let j = 0; j < prevOutputs.length; j++) {
                            sum += prevOutputs[j] * this.hiddenLayers[k].neurons[i].weights[j];
                        }

                        /**
                         * Calculate the output of the neuron using the activation function.
                         */
                        this.hiddenLayers[k].neurons[i].output = ActivationFunctions.apply(
                            this.hiddenLayers[k].activationType, sum
                        );

                        /**
                         * Add the output of the neuron to the current outputs array.
                         */
                        curOutputs.push(this.hiddenLayers[k].neurons[i].output);
                    }

                    /**
                     * Update the previous outputs with the current outputs.
                     */
                    prevOutputs = curOutputs;
                }

                /**
                 * Calculate the output of the output layer.
                 */
                let outputSums = [];
                for (let i = 0; i < this.outputLayer.neurons.length; i++) {
                    /**
                     * Initialize the sum of the weighted inputs to the neuron.
                     */
                    let sum = this.outputLayer.neurons[i].bias;

                    /**
                     * Calculate the weighted sum of the inputs to the neuron.
                     */
                    for (let j = 0; j < prevOutputs.length; j++) {
                        sum += prevOutputs[j] * this.outputLayer.neurons[i].weights[j];
                    }

                    /**
                     * Add the output of the neuron to the output sums array.
                     */
                    outputSums.push(sum);
                }

                /**
                 * Calculate the output of the neural network.
                 */
                let output;
                if (this.outputActivation === 'softmax') {
                    /**
                     * If the output activation function is softmax, calculate the output using the softmax function.
                     */
                    output = ActivationFunctions.softmax(outputSums);
                    /**
                     * Update the output of the output layer.
                     */
                    for (let i = 0; i < this.outputLayer.neurons.length; i++) {
                        this.outputLayer.neurons[i].output = output[i];
                    }
                } else {
                    /**
                     * If the output activation function is not softmax, calculate the output using the activation function.
                     */
                    output = [];
                    for (let i = 0; i < this.outputLayer.neurons.length; i++) {
                        /**
                         * Calculate the output of the neuron using the activation function.
                         */
                        this.outputLayer.neurons[i].output = ActivationFunctions.apply(
                            this.outputActivation, outputSums[i]
                        );

                        /**
                         * Add the output of the neuron to the output array.
                         */
                        output.push(this.outputLayer.neurons[i].output);
                    }
                }

                /**
                 * Return the output of the neural network.
                 */
                return output;
            }

            /**
             * Performs backpropagation to calculate error gradients for all neurons in the network.
             * This implements the backward pass of the backpropagation algorithm, computing how much
             * each neuron contributed to the final error and storing this in the neuron's error property.
             * @param {Array} target - The expected output values (target labels) for the current training example.
             */

            backPropagate(target) {
                // Calculate errors for output layer neurons
                if (this.outputActivation === 'softmax') {
                    // For softmax activation, use cross-entropy derivative
                    // This directly computes (target - predicted) for each output neuron
                    let derivatives = ActivationFunctions.dSoftmax(target,
                        this.outputLayer.neurons.map(n => n.output)
                    );
                    for (let i = 0; i < this.outputLayer.neurons.length; i++) {
                        this.outputLayer.neurons[i].error = derivatives[i];
                    }
                } else {
                    // For other activation functions, use standard error calculation
                    // Error = derivative(activation) * (target - actual)
                    for (let i = 0; i < this.outputLayer.neurons.length; i++) {
                        let o = this.outputLayer.neurons[i].output;
                        let derivative = ActivationFunctions.applyDerivative(this.outputActivation, o);
                        this.outputLayer.neurons[i].error = derivative * (target[i] - o);
                    }
                }

                // Propagate errors backward through hidden layers (from last to first)
                for (let k = this.hiddenLayers.length - 1; k >= 0; k--) {
                    for (let i = 0; i < this.hiddenLayers[k].neurons.length; i++) {
                        let neuron = this.hiddenLayers[k].neurons[i];
                        let sum = 0;

                        // Calculate weighted sum of errors from the next layer
                        if (k === this.hiddenLayers.length - 1) {
                            // This is the last hidden layer, so get errors from output layer
                            for (let j = 0; j < this.outputLayer.neurons.length; j++) {
                                sum += this.outputLayer.neurons[j].error * this.outputLayer.neurons[j].weights[i];
                            }
                        } else {
                            // Get errors from the next hidden layer
                            for (let j = 0; j < this.hiddenLayers[k + 1].neurons.length; j++) {
                                sum += this.hiddenLayers[k + 1].neurons[j].error * this.hiddenLayers[k + 1].neurons[j].weights[i];
                            }
                        }
                        // Calculate this neuron's error using the chain rule
                        // Error = derivative(activation) * weighted_sum_of_next_layer_errors
                        let derivative = ActivationFunctions.applyDerivative(
                            this.hiddenLayers[k].activationType, neuron.output
                        );
                        neuron.error = derivative * sum;
                    }
                }
            }
            /**
            * Updates all weights and biases in the network using the calculated error gradients.
            * This implements the weight update step of the backpropagation algorithm using gradient descent.
            * @param {Array} input - The original input values used for this training example.
            */
            updateWeights(input) {
                // Collect outputs from each layer for weight updates
                // layerOutputs[0] = input layer, layerOutputs[1] = first hidden layer, etc.
                let layerOutputs = [input];
                for (let k = 0; k < this.hiddenLayers.length; k++) {
                    let outputs = [];
                    for (let i = 0; i < this.hiddenLayers[k].neurons.length; i++) {
                        outputs.push(this.hiddenLayers[k].neurons[i].output);
                    }
                    layerOutputs.push(outputs);
                }
                
                // Update output layer weights and biases
                let lastHiddenOutputs = layerOutputs[layerOutputs.length - 1];
                for (let i = 0; i < this.outputLayer.neurons.length; i++) {
                    let neuron = this.outputLayer.neurons[i];
                    // Update each weight: weight += learning_rate * error * input_from_previous_layer
                    for (let j = 0; j < neuron.weights.length; j++) {
                        neuron.weights[j] += this.learningRate * neuron.error * lastHiddenOutputs[j];
                    }
                    // Update bias: bias += learning_rate * error
                    neuron.bias += this.learningRate * neuron.error;
                }

                // Update hidden layer weights and biases (from last to first)
                for (let k = this.hiddenLayers.length - 1; k >= 0; k--) {
                    let prevLayerOutputs = layerOutputs[k];
                    for (let i = 0; i < this.hiddenLayers[k].neurons.length; i++) {
                        let neuron = this.hiddenLayers[k].neurons[i];
                        // Update each weight using gradient descent
                        for (let j = 0; j < neuron.weights.length; j++) {
                            neuron.weights[j] += this.learningRate * neuron.error * prevLayerOutputs[j];
                        }
                        // Update bias
                        neuron.bias += this.learningRate * neuron.error;
                    }
                }
            }

            /**
            * Makes a prediction for the given input without updating weights.
            * This is used for inference after training is complete.
            * @param {Array} input - The input values to make a prediction for.
            * @returns {Array} The predicted output values from the network.
            */
            predict(input) {
                return this.feedForward(input);
            }

            /**
            * Performs one complete training step: forward pass, backward pass, and weight update.
            * This is the main training method that combines all phases of the learning process.
            * @param {Array} input - The input values for this training example.
            * @param {Array} target - The expected output values for this training example.
            */
            train(input, target) {
                this.feedForward(input);
                this.backPropagate(target);
                this.updateWeights(input);
            }
            /**
            * Serializes the entire neural network to a JSON string for saving.
            * This includes all network architecture, weights, biases, and configuration parameters.
            * @returns {String} JSON string representation of the complete network state.
            */
            toJSON() {
                return JSON.stringify({
                    learningRate: this.learningRate,
                    inputSize: this.inputLayer.neurons.length,
                    hiddenSizes: this.hiddenLayers.map(l => l.neurons.length),
                    outputSize: this.outputLayer.neurons.length,
                    hiddenActivation: this.hiddenActivation,
                    outputActivation: this.outputActivation,
                    hiddenLayers: this.hiddenLayers.map(layer => ({
                        activationType: layer.activationType,
                        neurons: layer.neurons.map(n => ({
                            weights: n.weights,
                            bias: n.bias,
                        }))
                    })),
                    outputLayer: {
                        activationType: this.outputLayer.activationType,
                        neurons: this.outputLayer.neurons.map(n => ({
                            weights: n.weights,
                            bias: n.bias,
                        }))
                    }
                });
            }
            /**
            * Creates a new TMultiLayerPerceptron instance from a JSON string.
            * This allows loading a previously saved network with all its trained weights and configuration.
            * @param {String} json - JSON string containing the serialized network data.
            * @returns {TMultiLayerPerceptron} A new MLP instance with the loaded configuration and weights.
            */
            static fromJSON(json) {
                let obj = JSON.parse(json);
                let mlp = new TMultiLayerPerceptron(
                    obj.inputSize,
                    obj.hiddenSizes,
                    obj.outputSize,
                    obj.hiddenActivation || 'sigmoid',
                    obj.outputActivation || 'sigmoid'
                );
                
                // Restore the learning rate
                mlp.learningRate = obj.learningRate;

                // Restore weights and biases for all hidden layers
                for (let k = 0; k < mlp.hiddenLayers.length; k++) {
                    if (obj.hiddenLayers[k].activationType) {
                        mlp.hiddenLayers[k].activationType = obj.hiddenLayers[k].activationType;
                    }
                    for (let i = 0; i < mlp.hiddenLayers[k].neurons.length; i++) {
                        mlp.hiddenLayers[k].neurons[i].weights = obj.hiddenLayers[k].neurons[i].weights;
                        mlp.hiddenLayers[k].neurons[i].bias = obj.hiddenLayers[k].neurons[i].bias;
                    }
                }

                // Restore weights and biases for output layer
                if (obj.outputLayer.activationType) {
                    mlp.outputLayer.activationType = obj.outputLayer.activationType;
                }
                for (let i = 0; i < mlp.outputLayer.neurons.length; i++) {
                    mlp.outputLayer.neurons[i].weights = obj.outputLayer.neurons[i].weights;
                    mlp.outputLayer.neurons[i].bias = obj.outputLayer.neurons[i].bias;
                }

                return mlp;
            }
        }

        // Globals
        let mlp = null;

        /**
        * Global array to store training data points.
        * Each element is a TDataPoint containing input and target arrays.
        * @type {Array<TDataPoint>}
        */
        let data = [];

        // Helper CSV
        /**
        * Parses CSV text data into an array of TDataPoint objects.
        * Expected format: each line contains input values followed by target values, comma-separated.
        * The number of input and output values is determined by the current UI settings.
        * @param {String} text - Raw CSV text with comma-separated values.
        * @returns {Array<TDataPoint>} Array of parsed data points ready for training.
        */
        function parseCSV(text) {
            let lines = text.trim().split("\n");
            let points = [];
            for (let line of lines) {
                if (line.trim() === "") continue;
                
                // Extract input and target portions from the CSV line
                let arr = line.trim().split(",").map(Number);
                let inputSize = parseInt(document.getElementById("inputSize").value);
                let outputSize = parseInt(document.getElementById("outputSize").value);
                let input = arr.slice(0, inputSize);
                let target = arr.slice(inputSize, inputSize + outputSize);

                // Only add valid data points with correct dimensions
                if (input.length === inputSize && target.length === outputSize) {
                    points.push(new TDataPoint(input, target));
                }
            }
            return points;
        }

        // UI Functions
        /**
        * Creates a new neural network based on current UI configuration settings.
        * Reads all network parameters from the HTML form and initializes a new MLP instance.
        * Updates the UI to show network creation status.
        */
        function createNetwork() {
            let inputSize = parseInt(document.getElementById("inputSize").value);
            let hiddenSizes = document.getElementById("hiddenSizes").value
                .split(",").map(s => parseInt(s.trim())).filter(n => !isNaN(n));
            let outputSize = parseInt(document.getElementById("outputSize").value);
            let learningRate = parseFloat(document.getElementById("learningRate").value);
            let hiddenActivation = document.getElementById("hiddenActivation").value;
            let outputActivation = document.getElementById("outputActivation").value;

            mlp = new TMultiLayerPerceptron(inputSize, hiddenSizes, outputSize, hiddenActivation, outputActivation);
            mlp.learningRate = learningRate;
            document.getElementById("networkStatus").innerText =
                `Network created! Hidden: ${hiddenActivation}, Output: ${outputActivation}`;
        }

        function generateTestData() {
            let samplesPerClass = parseInt(document.getElementById("samplesPerClass").value);
            let inputSize = parseInt(document.getElementById("inputSize").value);
            let outputSize = parseInt(document.getElementById("outputSize").value);

            data = MLEvaluation.generateTestData(samplesPerClass, inputSize, outputSize);
            document.getElementById("generateStatus").innerText =
                `Generated ${data.length} test samples (${samplesPerClass} per class)`;
        }

        function runKFoldValidation() {
            if (!mlp) { alert("Create a network first."); return; }
            if (data.length === 0) { alert("Load or generate training data first."); return; }

            let numFolds = parseInt(document.getElementById("numFolds").value);
            let validationEpochs = parseInt(document.getElementById("validationEpochs").value);
            let resultsDiv = document.getElementById("evaluationResults");
            resultsDiv.innerHTML = `<p>Running K-Fold Cross Validation with ${validationEpochs} epochs per fold...</p>`;

            setTimeout(() => {
                let accuracy = MLEvaluation.kFoldCrossValidation(data, numFolds, mlp, validationEpochs);

                resultsDiv.innerHTML = `
                    <div class="results">
                        <h3>K-Fold Cross Validation Results (${numFolds} folds, ${validationEpochs} epochs each)</h3>
                        <div class="metric">Accuracy: ${(accuracy * 100).toFixed(2)}%</div>
                    </div>
                `;
            }, 100);
        }

        function runAllMetrics() {
            if (!mlp) { alert("Create a network first."); return; }
            if (data.length === 0) { alert("Load or generate training data first."); return; }

            let resultsDiv = document.getElementById("evaluationResults");
            let trainingEpochs = parseInt(document.getElementById("trainingEpochs").value);
            let validationEpochs = parseInt(document.getElementById("validationEpochs").value);

            resultsDiv.innerHTML = `<p>Running all evaluation metrics with ${trainingEpochs} training epochs...</p>`;

            setTimeout(() => {
                let numFolds = parseInt(document.getElementById("numFolds").value);
                let batchSize = document.getElementById("batchSize").value;

                // Train the model first with specified epochs and batch size
                MLEvaluation.trainWithBatches(mlp, data, trainingEpochs, batchSize);

                let outputSize = parseInt(document.getElementById("outputSize").value);
                let results = [];

                for (let classIndex = 0; classIndex < outputSize; classIndex++) {
                    let precision = MLEvaluation.precisionScore(data, mlp, classIndex);
                    let recall = MLEvaluation.recallScore(data, mlp, classIndex);
                    let f1 = MLEvaluation.f1Score(precision, recall);

                    results.push({
                        class: classIndex,
                        precision: precision,
                        recall: recall,
                        f1: f1
                    });
                }

                let accuracy = MLEvaluation.kFoldCrossValidation(data, numFolds, mlp, validationEpochs);

                let html = `
                    <div class="results">
                        <h3>Complete Evaluation Results</h3>
                        <div class="metric">Training Epochs: ${trainingEpochs}</div>
                        <div class="metric">Validation Epochs per Fold: ${validationEpochs}</div>
                        <div class="metric">Batch Size: ${batchSize}</div>
                        <div class="metric">Overall Accuracy: ${(accuracy * 100).toFixed(2)}%</div>

                        <table>
                            <tr>
                                <th>Class</th>
                                <th>Precision</th>
                                <th>Recall</th>
                                <th>F1 Score</th>
                            </tr>
                `;

                results.forEach(result => {
                    html += `
                        <tr>
                            <td>Class ${result.class}</td>
                            <td>${(result.precision * 100).toFixed(2)}%</td>
                            <td>${(result.recall * 100).toFixed(2)}%</td>
                            <td>${(result.f1 * 100).toFixed(2)}%</td>
                        </tr>
                    `;
                });

                // Calculate averages
                let avgPrecision = results.reduce((sum, r) => sum + r.precision, 0) / results.length;
                let avgRecall = results.reduce((sum, r) => sum + r.recall, 0) / results.length;
                let avgF1 = results.reduce((sum, r) => sum + r.f1, 0) / results.length;

                html += `
                        <tr style="font-weight: bold; background-color: #e8e8e8;">
                            <td>Average</td>
                            <td>${(avgPrecision * 100).toFixed(2)}%</td>
                            <td>${(avgRecall * 100).toFixed(2)}%</td>
                            <td>${(avgF1 * 100).toFixed(2)}%</td>
                        </tr>
                    </table>
                    </div>
                `;

                resultsDiv.innerHTML = html;
            }, 100);
        }

        function trainNetwork() {

            if (!mlp) { alert("Create a network first."); return; }
            if (data.length === 0) { alert("Load training data first."); return; }

            let trainingEpochs = parseInt(document.getElementById("trainingEpochs").value);
            let batchSize = document.getElementById("batchSize").value;
            let statusDiv = document.getElementById("trainStatus");

            statusDiv.innerText = `Training for ${trainingEpochs} epochs with batch size ${batchSize}...`;

            setTimeout(() => {
                MLEvaluation.trainWithBatches(mlp, data, trainingEpochs, batchSize);
                statusDiv.innerText = `Training completed: ${trainingEpochs} epochs over ${data.length} data points.`;
            }, 20);
        }

        function trainNetworkWithProgress() {
            if (!mlp) { alert("Create a network first."); return; }
            if (data.length === 0) { alert("Load training data first."); return; }

            let trainingEpochs = parseInt(document.getElementById("trainingEpochs").value);
            let batchSize = document.getElementById("batchSize").value;
            let statusDiv = document.getElementById("trainStatus");
            let progressDiv = document.getElementById("trainingProgress");

            statusDiv.innerText = `Training for ${trainingEpochs} epochs with batch size ${batchSize}...`;
            progressDiv.innerHTML = '<div>Epoch: 0 / ' + trainingEpochs + '</div>';

            setTimeout(() => {
                MLEvaluation.trainWithBatches(mlp, data, trainingEpochs, batchSize, (currentEpoch, totalEpochs) => {
                    progressDiv.innerHTML = `
                        <div>Epoch: ${currentEpoch} / ${totalEpochs}</div>
                        <div style="width: 300px; background-color: #f0f0f0; border-radius: 5px; margin-top: 5px;">
                            <div style="width: ${(currentEpoch / totalEpochs) * 100}%; background-color: #4CAF50; height: 20px; border-radius: 5px;"></div>
                        </div>
                        <div>${((currentEpoch / totalEpochs) * 100).toFixed(1)}% Complete</div>
                    `;
                });

                statusDiv.innerText = `Training completed: ${trainingEpochs} epochs over ${data.length} data points.`;
            }, 20);
        }

        function loadCSV() {
            let fileInput = document.getElementById("csvFile");
            if (fileInput.files.length === 0) return;
            let reader = new FileReader();
            reader.onload = function (e) {
                data = parseCSV(e.target.result);
                document.getElementById("dataStatus").innerText = `Loaded ${data.length} data points.`;
            };
            reader.readAsText(fileInput.files[0]);
        }

        function loadCSVText() {
            let text = document.getElementById("csvPaste").value;
            data = parseCSV(text);
            document.getElementById("dataStatus").innerText = `Loaded ${data.length} data points.`;
        }

        function saveModel() {
            if (!mlp) { alert("No model to save."); return; }
            let json = mlp.toJSON();
            let blob = new Blob([json], { type: "application/json" });
            let a = document.createElement("a");
            a.href = URL.createObjectURL(blob);
            a.download = "mlp-model.json";
            a.click();
            document.getElementById("modelStatus").innerText = "Model saved.";
        }

        function loadModel() {
            let fileInput = document.getElementById("modelFile");
            if (fileInput.files.length === 0) return;
            let reader = new FileReader();
            reader.onload = function (e) {
                mlp = TMultiLayerPerceptron.fromJSON(e.target.result);
                document.getElementById("modelStatus").innerText = "Model loaded!";
            };
            reader.readAsText(fileInput.files[0]);
        }

        function predictInput() {
            if (!mlp) { alert("Create/load network first."); return; }
            let input = document.getElementById("predictInput").value.split(",").map(Number);
            if (input.length !== mlp.inputLayer.neurons.length) {
                alert(`Input length must be ${mlp.inputLayer.neurons.length}`);
                return;
            }
            let output = mlp.predict(input);
            let predictedClass = MLEvaluation.getMaxIndex(output);
            document.getElementById("predictOutput").innerHTML =
                `<strong>Raw Output:</strong> ${output.map(x => x.toFixed(4)).join(", ")}<br>
                 <strong>Predicted Class:</strong> ${predictedClass} (confidence: ${(output[predictedClass] * 100).toFixed(1)}%)`;
        }

    </script>
</body>

</html>
